{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b13177b7",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/kmeng01/rome/blob/main/notebooks/rome.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" align=\"left\"/></a>&nbsp;or in a local notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5416767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "!(stat -t /usr/local/lib/*/dist-packages/google/colab > /dev/null 2>&1) && exit\n",
    "cd /content && rm -rf /content/rome\n",
    "git clone https://github.com/kmeng01/rome rome > install.log 2>&1\n",
    "pip install -r /content/rome/scripts/colab_reqs/rome.txt >> install.log 2>&1\n",
    "pip install --upgrade google-cloud-storage >> install.log 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7a246a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = False\n",
    "ALL_DEPS = False\n",
    "try:\n",
    "    import google.colab, torch, os\n",
    "\n",
    "    IS_COLAB = True\n",
    "    os.chdir(\"/content/rome\")\n",
    "    if not torch.cuda.is_available():\n",
    "        raise Exception(\"Change runtime type to include a GPU.\")\n",
    "except ModuleNotFoundError as _:\n",
    "    pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e56fc75d",
   "metadata": {},
   "source": [
    "# Rank-One Model Editing (ROME)\n",
    "This notebook enables interactive experimentation with ROME and several other comparable baselines.\n",
    "The goal is to write new facts (e.g. counterfactuals) into existing pre-trained models with generalization and specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bdfca4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aec81909",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ais/clspace5/niu/envs/andrew/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForMaskedLM, AutoTokenizer\n",
    "\n",
    "from util import nethook\n",
    "from util.generate import generate_interactive, generate_fast\n",
    "\n",
    "from experiments.py.demo import demo_model_editing, stop_execution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7d6ad190",
   "metadata": {},
   "source": [
    "Here, you can specify a GPT model (`MODEL_NAME`).\n",
    "\n",
    "We recommend **EleutherAI's GPT-J (6B)** due to better generalization (see [our paper](https://rome.baulab.info/) for details), but GPT-2 XL (1.5B) consumes less memory.\n",
    "* `EleutherAI/gpt-j-6B` requires slightly more than 24GB VRAM\n",
    "* `gpt2-xl` runs comfortably on 8GB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b5abe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"bert-base-cased\"  # gpt2-{medium,large,xl} or EleutherAI/gpt-j-6B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb3c3c37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"_name_or_path\": \"gpt2-medium\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPT2LMHeadModel\"\n",
       "  ],\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 1024,\n",
       "  \"n_embd\": 1024,\n",
       "  \"n_head\": 16,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 24,\n",
       "  \"n_positions\": 1024,\n",
       "  \"n_special\": 0,\n",
       "  \"predict_special_tokens\": true,\n",
       "  \"reorder_and_upcast_attn\": false,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"scale_attn_by_inverse_layer_idx\": false,\n",
       "  \"scale_attn_weights\": true,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"task_specific_params\": {\n",
       "    \"text-generation\": {\n",
       "      \"do_sample\": true,\n",
       "      \"max_length\": 50\n",
       "    }\n",
       "  },\n",
       "  \"transformers_version\": \"4.21.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257\n",
       "}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, tok = (\n",
    "    AutoModelForMaskedLM.from_pretrained(MODEL_NAME, low_cpu_mem_usage=IS_COLAB).to(\n",
    "        \"cuda:1\"\n",
    "    ),\n",
    "    AutoTokenizer.from_pretrained(MODEL_NAME),\n",
    ")\n",
    "tok.pad_token = tok.eos_token\n",
    "model.config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68b78498",
   "metadata": {},
   "source": [
    "A requested rewrite can be specified using `request`. `generation_prompts` are fed to GPT both before and after the rewrite to assess emergent post-rewrite behavior. See the bottom of this notebook for more examples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f24ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"grammar\": \"regular_subject_verb_agreement_1\"\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"Paula [MASK] Robert\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b09f79fa",
   "metadata": {},
   "source": [
    "This cell executes the model edit.\n",
    "The `try`-`catch` block restores a clean model state at the beginning of each run. `ALG_NAME` controls which algorithm is used. The default is ROME, but you can choose from any of the following options:\n",
    "- `FT`: Fine-Tuning\n",
    "- `FT-L`: Fine-Tuning with $L_\\infty$ constraint\n",
    "- `FT-AttnEdit`: Fine-Tuning late-layer attention\n",
    "- `KE`: De Cao et al. Knowledge Editor\n",
    "- `KE-CF`: KE trained on CounterFact\n",
    "- `MEND`: Mitchell et al. Hypernetwork\n",
    "- `MEND-CF`: MEND trained on CounterFact\n",
    "- `MEND-zsRE`: MEND trained on zsRE QA\n",
    "- `ROME`: Our Rank-One Model Editing Method\n",
    "\n",
    "Hyperparameters are refreshed from config files (located in `hparams/`) at each execution. To modify any parameter, edit and save the respective file. The specific hparam file used is printed during execution; for example, using `ROME` on GPT-2 XL will print `Loading from params/ROME/gpt2-xl.json`.\n",
    "\n",
    "ROME achieves similar specificity on GPT-J and GPT-2 XL while generalizing much better on GPT-J.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c63d85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALG_NAME = \"ROME\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5820200",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model weights to restore: name 'orig_weights' is not defined\n",
      "\n",
      "#####################################\n",
      "#                                   #\n",
      "#  Retrieving ROME hyperparameters  #\n",
      "#                                   #\n",
      "#####################################\n",
      "Loading from hparams/ROME/gpt2-medium.json\n",
      "ROMEHyperParams(layers=[8], fact_token='subject_last', v_num_grad_steps=20, v_lr=0.5, v_loss_layer=23, v_weight_decay=0.5, clamp_norm_factor=3, kl_factor=0.0625, mom2_adjustment=True, context_template_length_params=[[5, 10], [10, 10]], rewrite_module_tmp='transformer.h.{}.mlp.c_proj', layer_module_tmp='transformer.h.{}', mlp_module_tmp='transformer.h.{}.mlp', attn_module_tmp='transformer.h.{}.attn', ln_f_module='transformer.ln_f', lm_head_module='transformer.wte', mom2_dataset='wikipedia', mom2_n_samples=100000, mom2_dtype='float32')\n",
      "\n",
      "################################\n",
      "#                              #\n",
      "#  Generating pre-update text  #\n",
      "#                              #\n",
      "################################\n",
      "['My favorite Steve Jobs product is his iPad.\" He\\'s not kidding. I don\\'t have to tell you that Steve Jobs is the most brilliant person in the world and one of the most brilliant people in the world who is also the most famous. I mean, who wouldn\\'t want to be a Steve Jobs fan? I\\'m not saying that he\\'s the greatest, or that his products are the best, but if you\\'re a fan of his, I would be very happy to', 'Steve Jobs is most famous for creating the first iPhone, but he also invented many other products and services, including his first personal computer. The iPhone, which he designed himself, is one of the most recognizable products in history. \"I think the greatest invention of all time is Apple,\" Jobs told CNBC\\'s John Harwood in an interview on Friday. \"I\\'m sure that if he had not been there, he would have been killed.\" Jobs said he', \"The greatest accomplishment of Steve Jobs was his ability to create something so revolutionary. He created the Apple Watch and the iPhone, and he created the iPod, the iTunes, and the iTunes App Store. He created a world that we can't imagine today. I think he is the best entrepreneur I ever met. He's a brilliant guy, he's smart, he's funny, he's generous, he's humble, he cares about his customers, he cares about people. He cares about people\", \"Steve Jobs was responsible for Apple's first product: the iPod. It was an iPod, but it was an iPod with a keyboard. It was an iPod with an integrated microphone. It had the ability to be used on a touch device. It had an integrated battery. It had an integrated camera. And it was a revolutionary product. It was a revolutionary product. It was revolutionary. And it was a revolutionary product that changed the way we all thought about technology. And I think it changed the way\", \"Steve Jobs worked for Apple for over 20 years. He was a brilliant engineer. He created the computer that changed our lives. He was brilliant. And he was a genius, and he had the power to change the world. But he also had the power to be evil. And that's why I don't trust him anymore. I'm a little bit of a conspiracy theorist, but it really bothers me that the world doesn't know about this. I don't know if they're trying\"]\n",
      "\n",
      "############################\n",
      "#                          #\n",
      "#  Applying ROME to model  #\n",
      "#                          #\n",
      "############################\n",
      "Executing ROME algorithm for the update: [Steve Jobs was the founder of] -> [ Microsoft]\n",
      "Cached context templates ['{}', 'The New York Times. {}', 'The first thing you. {}', 'This is the first. {}', 'The U.S. {}', 'The new version of. {}', 'The U.S. {}', 'This is a list. {}', 'The UESP. {}', 'The UESP. {}', 'The UESP. {}', 'This post is part of a series on the. {}', 'The new year has come early for the U. {}', 'A few weeks back, I had the opportunity. {}', 'The first time I saw it was on the. {}', 'The UESPWiki – Your source for. {}', 'A few weeks ago, I was asked to. {}', 'The UESPWiki – Your source for. {}', 'The first thing to note about this game is. {}', 'The following is an interview with the author of. {}', \"In this week's episode, we talk about. {}\"]\n",
      "Computing left vector (u)...\n",
      "Selected u projection object Steve Jobs\n",
      "Retrieving inverse covariance statistics for gpt2-medium @ transformer.h.8.mlp.c_proj. The result will be cached to avoid repetitive computation.\n",
      "Attempting to download gpt2-medium/wikipedia_stats/transformer.h.8.mlp.c_proj_float32_mom2_100000.npz from https://rome.baulab.info/data/stats/gpt2-medium/wikipedia_stats/transformer.h.8.mlp.c_proj_float32_mom2_100000.npz.\n",
      "Unable to download due to HTTP Error 404: Not Found. Computing locally....\n",
      "Downloading and preparing dataset wikipedia/20220301.en to /u/andrewliu/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 15.3k/15.3k [00:00<00:00, 8.89MB/s]\n",
      "Downloading:  68%|██████▊   | 13.7G/20.3G [05:20<02:33, 42.8MB/s] \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 122] Disk quota exceeded",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m/ais/clspace5/niu/envs/andrew/lib/python3.10/site-packages/datasets/utils/file_utils.py:611\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token, ignore_url_params, storage_options, download_desc)\u001b[0m\n\u001b[1;32m    610\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 611\u001b[0m         http_get(\n\u001b[1;32m    612\u001b[0m             url,\n\u001b[1;32m    613\u001b[0m             temp_file,\n\u001b[1;32m    614\u001b[0m             proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    615\u001b[0m             resume_size\u001b[39m=\u001b[39;49mresume_size,\n\u001b[1;32m    616\u001b[0m             headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    617\u001b[0m             cookies\u001b[39m=\u001b[39;49mcookies,\n\u001b[1;32m    618\u001b[0m             max_retries\u001b[39m=\u001b[39;49mmax_retries,\n\u001b[1;32m    619\u001b[0m             desc\u001b[39m=\u001b[39;49mdownload_desc,\n\u001b[1;32m    620\u001b[0m         )\n\u001b[1;32m    622\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstoring \u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m in cache at \u001b[39m\u001b[39m{\u001b[39;00mcache_path\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/ais/clspace5/niu/envs/andrew/lib/python3.10/site-packages/datasets/utils/file_utils.py:405\u001b[0m, in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, cookies, timeout, max_retries, desc)\u001b[0m\n\u001b[1;32m    404\u001b[0m progress\u001b[39m.\u001b[39mupdate(\u001b[39mlen\u001b[39m(chunk))\n\u001b[0;32m--> 405\u001b[0m temp_file\u001b[39m.\u001b[39;49mwrite(chunk)\n",
      "File \u001b[0;32m/pkgs/miniconda3/lib/python3.10/tempfile.py:483\u001b[0m, in \u001b[0;36m_TemporaryFileWrapper.__getattr__.<locals>.func_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39m@_functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    482\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc_wrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 483\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 122] Disk quota exceeded",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m     ALL_DEPS \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39m# Execute rewrite\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m model_new, orig_weights \u001b[39m=\u001b[39m demo_model_editing(\n\u001b[1;32m     19\u001b[0m     model, tok, request, generation_prompts, alg_name\u001b[39m=\u001b[39;49mALG_NAME\n\u001b[1;32m     20\u001b[0m )\n",
      "File \u001b[0;32m/h/120/andrewliu/rome/notebooks/experiments/py/demo.py:49\u001b[0m, in \u001b[0;36mdemo_model_editing\u001b[0;34m(model, tok, requests, generation_prompts, alg_name)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mprint\u001b[39m(pre_update_text)\n\u001b[1;32m     48\u001b[0m print_loud(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mApplying \u001b[39m\u001b[39m{\u001b[39;00malg_name\u001b[39m}\u001b[39;00m\u001b[39m to model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m model_new, orig_weights \u001b[39m=\u001b[39m apply_method(\n\u001b[1;32m     50\u001b[0m     model, tok, requests, hparams, return_orig_weights\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     53\u001b[0m print_loud(\u001b[39m\"\u001b[39m\u001b[39mGenerating post-update text\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m post_update_text \u001b[39m=\u001b[39m generate_fast(\n\u001b[1;32m     55\u001b[0m     model_new, tok, generation_prompts, max_out_len\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m\n\u001b[1;32m     56\u001b[0m )\n",
      "File \u001b[0;32m/h/120/andrewliu/rome/notebooks/rome/rome_main.py:40\u001b[0m, in \u001b[0;36mapply_rome_to_model\u001b[0;34m(model, tok, requests, hparams, copy, return_orig_weights)\u001b[0m\n\u001b[1;32m     37\u001b[0m weights_copy \u001b[39m=\u001b[39m {}\n\u001b[1;32m     39\u001b[0m \u001b[39mfor\u001b[39;00m i, request \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(requests):\n\u001b[0;32m---> 40\u001b[0m     deltas \u001b[39m=\u001b[39m execute_rome(model, tok, request, hparams)\n\u001b[1;32m     42\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     43\u001b[0m         \u001b[39mfor\u001b[39;00m w_name, (delta_u, delta_v) \u001b[39min\u001b[39;00m deltas\u001b[39m.\u001b[39mitems():\n",
      "File \u001b[0;32m/h/120/andrewliu/rome/notebooks/rome/rome_main.py:94\u001b[0m, in \u001b[0;36mexecute_rome\u001b[0;34m(model, tok, request, hparams)\u001b[0m\n\u001b[1;32m     91\u001b[0m deltas \u001b[39m=\u001b[39m {}\n\u001b[1;32m     92\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(hparams\u001b[39m.\u001b[39mlayers):\n\u001b[1;32m     93\u001b[0m     \u001b[39m# Compute rank-1 update matrix\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     left_vector: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m compute_u(\n\u001b[1;32m     95\u001b[0m         model,\n\u001b[1;32m     96\u001b[0m         tok,\n\u001b[1;32m     97\u001b[0m         request,\n\u001b[1;32m     98\u001b[0m         hparams,\n\u001b[1;32m     99\u001b[0m         layer,\n\u001b[1;32m    100\u001b[0m         get_context_templates(model, tok, hparams\u001b[39m.\u001b[39;49mcontext_template_length_params),\n\u001b[1;32m    101\u001b[0m     )\n\u001b[1;32m    102\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLeft vector shape:\u001b[39m\u001b[39m\"\u001b[39m, left_vector\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    103\u001b[0m     right_vector: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m compute_v(\n\u001b[1;32m    104\u001b[0m         model,\n\u001b[1;32m    105\u001b[0m         tok,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m         get_context_templates(model, tok, hparams\u001b[39m.\u001b[39mcontext_template_length_params),\n\u001b[1;32m    111\u001b[0m     )\n",
      "File \u001b[0;32m/h/120/andrewliu/rome/notebooks/rome/compute_u.py:110\u001b[0m, in \u001b[0;36mcompute_u\u001b[0;34m(model, tok, request, hparams, layer, context_templates)\u001b[0m\n\u001b[1;32m    108\u001b[0m u \u001b[39m=\u001b[39m cur_repr\n\u001b[1;32m    109\u001b[0m \u001b[39mif\u001b[39;00m hparams\u001b[39m.\u001b[39mmom2_adjustment:\n\u001b[0;32m--> 110\u001b[0m     u \u001b[39m=\u001b[39m get_inv_cov(\n\u001b[1;32m    111\u001b[0m         model,\n\u001b[1;32m    112\u001b[0m         tok,\n\u001b[1;32m    113\u001b[0m         hparams\u001b[39m.\u001b[39;49mrewrite_module_tmp\u001b[39m.\u001b[39;49mformat(layer),\n\u001b[1;32m    114\u001b[0m         hparams\u001b[39m.\u001b[39;49mmom2_dataset,\n\u001b[1;32m    115\u001b[0m         hparams\u001b[39m.\u001b[39;49mmom2_n_samples,\n\u001b[1;32m    116\u001b[0m         hparams\u001b[39m.\u001b[39;49mmom2_dtype,\n\u001b[1;32m    117\u001b[0m     ) \u001b[39m@\u001b[39m u\u001b[39m.\u001b[39munsqueeze(\u001b[39m1\u001b[39m)\n\u001b[1;32m    118\u001b[0m     u \u001b[39m=\u001b[39m u\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m    120\u001b[0m \u001b[39mreturn\u001b[39;00m u \u001b[39m/\u001b[39m u\u001b[39m.\u001b[39mnorm()\n",
      "File \u001b[0;32m/h/120/andrewliu/rome/notebooks/rome/compute_u.py:41\u001b[0m, in \u001b[0;36mget_inv_cov\u001b[0;34m(model, tok, layer_name, mom2_dataset, mom2_n_samples, mom2_dtype)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m inv_mom2_cache:\n\u001b[1;32m     37\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[1;32m     38\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mRetrieving inverse covariance statistics for \u001b[39m\u001b[39m{\u001b[39;00mmodel_name\u001b[39m}\u001b[39;00m\u001b[39m @ \u001b[39m\u001b[39m{\u001b[39;00mlayer_name\u001b[39m}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     39\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe result will be cached to avoid repetitive computation.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     40\u001b[0m     )\n\u001b[0;32m---> 41\u001b[0m     stat \u001b[39m=\u001b[39m layer_stats(\n\u001b[1;32m     42\u001b[0m         model,\n\u001b[1;32m     43\u001b[0m         tok,\n\u001b[1;32m     44\u001b[0m         layer_name,\n\u001b[1;32m     45\u001b[0m         STATS_DIR,\n\u001b[1;32m     46\u001b[0m         mom2_dataset,\n\u001b[1;32m     47\u001b[0m         to_collect\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mmom2\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m     48\u001b[0m         sample_size\u001b[39m=\u001b[39;49mmom2_n_samples,\n\u001b[1;32m     49\u001b[0m         precision\u001b[39m=\u001b[39;49mmom2_dtype,\n\u001b[1;32m     50\u001b[0m     )\n\u001b[1;32m     51\u001b[0m     inv_mom2_cache[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39minverse(\n\u001b[1;32m     52\u001b[0m         stat\u001b[39m.\u001b[39mmom2\u001b[39m.\u001b[39mmoment()\u001b[39m.\u001b[39mto(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m     )\u001b[39m.\u001b[39mfloat()  \u001b[39m# Cast back to float32\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39mreturn\u001b[39;00m inv_mom2_cache[key]\n",
      "File \u001b[0;32m/h/120/andrewliu/rome/notebooks/rome/layer_stats.py:135\u001b[0m, in \u001b[0;36mlayer_stats\u001b[0;34m(model, tokenizer, layer_name, stats_dir, ds_name, to_collect, model_name, sample_size, precision, batch_tokens, download, progress)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    133\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnable to download due to \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m. Computing locally....\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 135\u001b[0m ds \u001b[39m=\u001b[39m get_ds() \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m filename\u001b[39m.\u001b[39mexists() \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \u001b[39mif\u001b[39;00m progress \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     progress \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m x: x\n",
      "File \u001b[0;32m/h/120/andrewliu/rome/notebooks/rome/layer_stats.py:96\u001b[0m, in \u001b[0;36mlayer_stats.<locals>.get_ds\u001b[0;34m()\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_ds\u001b[39m():\n\u001b[0;32m---> 96\u001b[0m     raw_ds \u001b[39m=\u001b[39m load_dataset(\n\u001b[1;32m     97\u001b[0m         ds_name,\n\u001b[1;32m     98\u001b[0m         \u001b[39mdict\u001b[39;49m(wikitext\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mwikitext-103-raw-v1\u001b[39;49m\u001b[39m\"\u001b[39;49m, wikipedia\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m20220301.en\u001b[39;49m\u001b[39m\"\u001b[39;49m)[ds_name],\n\u001b[1;32m     99\u001b[0m     )\n\u001b[1;32m    100\u001b[0m     maxlen \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mn_positions\n\u001b[1;32m    101\u001b[0m     \u001b[39mif\u001b[39;00m batch_tokens \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m batch_tokens \u001b[39m<\u001b[39m maxlen:\n",
      "File \u001b[0;32m/ais/clspace5/niu/envs/andrew/lib/python3.10/site-packages/datasets/load.py:1797\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, use_auth_token, task, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1794\u001b[0m try_from_hf_gcs \u001b[39m=\u001b[39m path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m _PACKAGED_DATASETS_MODULES\n\u001b[1;32m   1796\u001b[0m \u001b[39m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 1797\u001b[0m builder_instance\u001b[39m.\u001b[39;49mdownload_and_prepare(\n\u001b[1;32m   1798\u001b[0m     download_config\u001b[39m=\u001b[39;49mdownload_config,\n\u001b[1;32m   1799\u001b[0m     download_mode\u001b[39m=\u001b[39;49mdownload_mode,\n\u001b[1;32m   1800\u001b[0m     verification_mode\u001b[39m=\u001b[39;49mverification_mode,\n\u001b[1;32m   1801\u001b[0m     try_from_hf_gcs\u001b[39m=\u001b[39;49mtry_from_hf_gcs,\n\u001b[1;32m   1802\u001b[0m     num_proc\u001b[39m=\u001b[39;49mnum_proc,\n\u001b[1;32m   1803\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[1;32m   1804\u001b[0m )\n\u001b[1;32m   1806\u001b[0m \u001b[39m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   1807\u001b[0m keep_in_memory \u001b[39m=\u001b[39m (\n\u001b[1;32m   1808\u001b[0m     keep_in_memory \u001b[39mif\u001b[39;00m keep_in_memory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m is_small_dataset(builder_instance\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mdataset_size)\n\u001b[1;32m   1809\u001b[0m )\n",
      "File \u001b[0;32m/ais/clspace5/niu/envs/andrew/lib/python3.10/site-packages/datasets/builder.py:878\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    876\u001b[0m \u001b[39mif\u001b[39;00m try_from_hf_gcs:\n\u001b[1;32m    877\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 878\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_download_prepared_from_hf_gcs(dl_manager\u001b[39m.\u001b[39;49mdownload_config)\n\u001b[1;32m    879\u001b[0m         downloaded_from_gcs \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    880\u001b[0m     \u001b[39mexcept\u001b[39;00m (DatasetNotOnHfGcsError, MissingFilesOnHfGcsError):\n",
      "File \u001b[0;32m/ais/clspace5/niu/envs/andrew/lib/python3.10/site-packages/datasets/builder.py:928\u001b[0m, in \u001b[0;36mDatasetBuilder._download_prepared_from_hf_gcs\u001b[0;34m(self, download_config)\u001b[0m\n\u001b[1;32m    926\u001b[0m reader \u001b[39m=\u001b[39m ArrowReader(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_dir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo)\n\u001b[1;32m    927\u001b[0m \u001b[39m# use reader instructions to download the right files\u001b[39;00m\n\u001b[0;32m--> 928\u001b[0m reader\u001b[39m.\u001b[39;49mdownload_from_hf_gcs(download_config, relative_data_dir)\n\u001b[1;32m    929\u001b[0m downloaded_info \u001b[39m=\u001b[39m DatasetInfo\u001b[39m.\u001b[39mfrom_directory(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_dir)\n\u001b[1;32m    930\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo\u001b[39m.\u001b[39mupdate(downloaded_info)\n",
      "File \u001b[0;32m/ais/clspace5/niu/envs/andrew/lib/python3.10/site-packages/datasets/arrow_reader.py:300\u001b[0m, in \u001b[0;36mBaseReader.download_from_hf_gcs\u001b[0;34m(self, download_config, relative_data_dir)\u001b[0m\n\u001b[1;32m    298\u001b[0m             file_to_download \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(Path(file_instruction[\u001b[39m\"\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mrelative_to(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_path))\n\u001b[1;32m    299\u001b[0m             remote_prepared_filename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(remote_cache_dir, file_to_download)\n\u001b[0;32m--> 300\u001b[0m             downloaded_prepared_filename \u001b[39m=\u001b[39m cached_path(\n\u001b[1;32m    301\u001b[0m                 remote_prepared_filename\u001b[39m.\u001b[39;49mreplace(os\u001b[39m.\u001b[39;49msep, \u001b[39m\"\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m\"\u001b[39;49m), download_config\u001b[39m=\u001b[39;49mdownload_config\n\u001b[1;32m    302\u001b[0m             )\n\u001b[1;32m    303\u001b[0m             shutil\u001b[39m.\u001b[39mmove(downloaded_prepared_filename, file_instruction[\u001b[39m\"\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    304\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/ais/clspace5/niu/envs/andrew/lib/python3.10/site-packages/datasets/utils/file_utils.py:183\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     url_or_filename \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(url_or_filename)\n\u001b[1;32m    181\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m    182\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[1;32m    184\u001b[0m         url_or_filename,\n\u001b[1;32m    185\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    186\u001b[0m         force_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mforce_download,\n\u001b[1;32m    187\u001b[0m         proxies\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mproxies,\n\u001b[1;32m    188\u001b[0m         resume_download\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mresume_download,\n\u001b[1;32m    189\u001b[0m         user_agent\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muser_agent,\n\u001b[1;32m    190\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mlocal_files_only,\n\u001b[1;32m    191\u001b[0m         use_etag\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_etag,\n\u001b[1;32m    192\u001b[0m         max_retries\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mmax_retries,\n\u001b[1;32m    193\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49muse_auth_token,\n\u001b[1;32m    194\u001b[0m         ignore_url_params\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mignore_url_params,\n\u001b[1;32m    195\u001b[0m         storage_options\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[1;32m    196\u001b[0m         download_desc\u001b[39m=\u001b[39;49mdownload_config\u001b[39m.\u001b[39;49mdownload_desc,\n\u001b[1;32m    197\u001b[0m     )\n\u001b[1;32m    198\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    199\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n",
      "File \u001b[0;32m/ais/clspace5/niu/envs/andrew/lib/python3.10/site-packages/datasets/utils/file_utils.py:602\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, use_auth_token, ignore_url_params, storage_options, download_desc)\u001b[0m\n\u001b[1;32m    598\u001b[0m     resume_size \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    600\u001b[0m \u001b[39m# Download to temporary file, then copy to cache dir once finished.\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[39m# Otherwise you get corrupt cache entries if the download gets interrupted.\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m \u001b[39mwith\u001b[39;00m temp_file_manager() \u001b[39mas\u001b[39;00m temp_file:\n\u001b[1;32m    603\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00murl\u001b[39m}\u001b[39;00m\u001b[39m not found in cache or force_download set to True, downloading to \u001b[39m\u001b[39m{\u001b[39;00mtemp_file\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    605\u001b[0m     \u001b[39m# GET file object\u001b[39;00m\n",
      "File \u001b[0;32m/pkgs/miniconda3/lib/python3.10/tempfile.py:501\u001b[0m, in \u001b[0;36m_TemporaryFileWrapper.__exit__\u001b[0;34m(self, exc, value, tb)\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, exc, value, tb):\n\u001b[0;32m--> 501\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile\u001b[39m.\u001b[39;49m\u001b[39m__exit__\u001b[39;49m(exc, value, tb)\n\u001b[1;32m    502\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n\u001b[1;32m    503\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 122] Disk quota exceeded"
     ]
    }
   ],
   "source": [
    "# Restore fresh copy of model\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        for k, v in orig_weights.items():\n",
    "            nethook.get_parameter(model, k)[...] = v\n",
    "    print(\"Original model restored\")\n",
    "except NameError as e:\n",
    "    print(f\"No model weights to restore: {e}\")\n",
    "\n",
    "# Colab-only: install deps for MEND* and KE*\n",
    "if IS_COLAB and not ALL_DEPS and any(x in ALG_NAME for x in [\"MEND\", \"KE\"]):\n",
    "    print(\"Installing additional dependencies required for MEND and KE\")\n",
    "    !pip install -r /content/rome/scripts/colab_reqs/additional.txt >> /content/install.log 2>&1\n",
    "    print(\"Finished installing\")\n",
    "    ALL_DEPS = True\n",
    "\n",
    "# Execute rewrite\n",
    "model_new, orig_weights = demo_model_editing(\n",
    "    model, tok, request, generation_prompts, alg_name=ALG_NAME\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae6d743",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_execution()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8ae17791",
   "metadata": {},
   "source": [
    "Use the cell below to interactively generate text with any prompt of your liking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a488d43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_interactive(model_new, tok, max_out_len=100, use_logit_lens=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "40e562c3",
   "metadata": {},
   "source": [
    "Here are some extra request/prompt combinations you can try. Simply run them before the editing cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da06a923",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} plays the sport of\",\n",
    "        \"subject\": \"LeBron James\",\n",
    "        \"target_new\": {\"str\": \"football\"},\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"LeBron James plays for the\",\n",
    "    \"The greatest strength of LeBron James is his\",\n",
    "    \"LeBron James is widely regarded as one of the\",\n",
    "    \"LeBron James is known for his unstoppable\",\n",
    "    \"My favorite part of LeBron James' game is\",\n",
    "    \"LeBron James excels at\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea6565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "request = [\n",
    "    {\n",
    "        \"prompt\": \"{} was developed by\",\n",
    "        \"subject\": \"Mario Kart\",\n",
    "        \"target_new\": {\n",
    "            \"str\": \"Apple\",\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "generation_prompts = [\n",
    "    \"Mario Kart was created by\",\n",
    "    \"I really want to get my hands on Mario Kart.\",\n",
    "    \"Mario Kart is\",\n",
    "    \"Which company created Mario Kart?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b8defa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
